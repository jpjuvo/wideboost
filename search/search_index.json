{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to wideboost! wideboost implements Wide Boosting as described in this article . wideboost does this by wrapping existing popular Gradient Boosting packages ( XGBoost and LightGBM ). If you're familiar with those packages wideboost can essentially be used as a drop-in replacement for either XGBoost or LightGBM when you're using one of the wideboost supported objective functions. Wide Boosting tweaks the usual Gradient Boosting framework as described in our Overview . It solves the same problems as Gradient Boosting. On several datasets is exhibits much better performance (see the Overview or the article ). Supported objective functions for wideboost include usual univariate and multivariate regression and binary and multi-category classification. Since Wide Boosting is closely related to Gradient Boosting, we can use the same tools to interpret a wideboost model. wideboost includes a wrapper on SHAP to aid in interpreting a wideboost model. Installation Overview Examples Supported objective functions XGBoost wrapper LightGBM wrapper SHAP explainer Reference Horrell, M. (2020). Wide Boosting. arXiv preprint arXiv:2007.09855.","title":"Home"},{"location":"#welcome-to-wideboost","text":"wideboost implements Wide Boosting as described in this article . wideboost does this by wrapping existing popular Gradient Boosting packages ( XGBoost and LightGBM ). If you're familiar with those packages wideboost can essentially be used as a drop-in replacement for either XGBoost or LightGBM when you're using one of the wideboost supported objective functions. Wide Boosting tweaks the usual Gradient Boosting framework as described in our Overview . It solves the same problems as Gradient Boosting. On several datasets is exhibits much better performance (see the Overview or the article ). Supported objective functions for wideboost include usual univariate and multivariate regression and binary and multi-category classification. Since Wide Boosting is closely related to Gradient Boosting, we can use the same tools to interpret a wideboost model. wideboost includes a wrapper on SHAP to aid in interpreting a wideboost model. Installation Overview Examples Supported objective functions XGBoost wrapper LightGBM wrapper SHAP explainer","title":"Welcome to wideboost!"},{"location":"#reference","text":"Horrell, M. (2020). Wide Boosting. arXiv preprint arXiv:2007.09855.","title":"Reference"},{"location":"examples/","text":"Examples XGBoost Example import xgboost as xgb from wideboost.wrappers import wxgb from pydataset import data import numpy as np ######## ## Get and format the data DAT = np.asarray(data('Yogurt')) X = DAT[:,0:9] Y = np.zeros([X.shape[0],1]) Y[DAT[:,9] == 'dannon'] = 1 Y[DAT[:,9] == 'hiland'] = 2 Y[DAT[:,9] == 'weight'] = 3 n = X.shape[0] np.random.seed(123) train_idx = np.random.choice(np.arange(n),round(n*0.5),replace=False) test_idx = np.setdiff1d(np.arange(n),train_idx) dtrain = xgb.DMatrix(X[train_idx,:],label=Y[train_idx,:]) dtest = xgb.DMatrix(X[test_idx,:],label=Y[test_idx,:]) ######### ######### ## Set parameters and run wide boosting param = {'btype':'I', ## wideboost param -- one of 'I', 'In', 'R', 'Rn' 'extra_dims':10, ## wideboost param -- integer >= 0 'max_depth':8, 'eta':0.1, 'objective':'multi:softmax', 'num_class':4, 'eval_metric':['merror'] } num_round = 100 watchlist = [(dtrain,'train'),(dtest,'test')] wxgb_results = dict() bst = wxgb.train(param, dtrain, num_round,watchlist,evals_result=wxgb_results) LightGBM Example import lightgbm as lgb from wideboost.wrappers import wlgb from pydataset import data import numpy as np ######## ## Get and format the data DAT = np.asarray(data('Yogurt')) X = DAT[:,0:9] Y = np.zeros([X.shape[0],1]) Y[DAT[:,9] == 'dannon'] = 1 Y[DAT[:,9] == 'hiland'] = 2 Y[DAT[:,9] == 'weight'] = 3 n = X.shape[0] np.random.seed(123) train_idx = np.random.choice(np.arange(n),round(n*0.5),replace=False) test_idx = np.setdiff1d(np.arange(n),train_idx) train_data = lgb.Dataset(X[train_idx,:],label=Y[train_idx,0]) test_data = lgb.Dataset(X[test_idx,:],label=Y[test_idx,0]) ######### ######### ## Set parameters and run wide boosting param = {'btype':'I', ## wideboost param -- one of 'I', 'In', 'R', 'Rn' 'extra_dims':10, ## wideboost param -- integer >= 0 'objective':'multiclass', 'metric':'multi_error', 'num_class':4, 'learning_rate': 0.1 } wlgb_results = dict() bst = wlgb.train(param, train_data, valid_sets=test_data, num_boost_round=100, evals_result=wlgb_results) SHAP Example from wideboost.explainers.shap import WTreeExplainer import shap explainer = WTreeExplainer(bst) shap_values = explainer.shap_values(data('Yogurt').iloc[0:1,0:9]) shap.initjs() print(bst.predict(xgb.DMatrix(np.asarray(data('Yogurt'))[0:1,0:9]))) shap.force_plot(explainer.expected_value[3],shap_values[3][0,:],data('Yogurt').iloc[0,0:9]) Screenshot of output:","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#xgboost-example","text":"import xgboost as xgb from wideboost.wrappers import wxgb from pydataset import data import numpy as np ######## ## Get and format the data DAT = np.asarray(data('Yogurt')) X = DAT[:,0:9] Y = np.zeros([X.shape[0],1]) Y[DAT[:,9] == 'dannon'] = 1 Y[DAT[:,9] == 'hiland'] = 2 Y[DAT[:,9] == 'weight'] = 3 n = X.shape[0] np.random.seed(123) train_idx = np.random.choice(np.arange(n),round(n*0.5),replace=False) test_idx = np.setdiff1d(np.arange(n),train_idx) dtrain = xgb.DMatrix(X[train_idx,:],label=Y[train_idx,:]) dtest = xgb.DMatrix(X[test_idx,:],label=Y[test_idx,:]) ######### ######### ## Set parameters and run wide boosting param = {'btype':'I', ## wideboost param -- one of 'I', 'In', 'R', 'Rn' 'extra_dims':10, ## wideboost param -- integer >= 0 'max_depth':8, 'eta':0.1, 'objective':'multi:softmax', 'num_class':4, 'eval_metric':['merror'] } num_round = 100 watchlist = [(dtrain,'train'),(dtest,'test')] wxgb_results = dict() bst = wxgb.train(param, dtrain, num_round,watchlist,evals_result=wxgb_results)","title":"XGBoost Example"},{"location":"examples/#lightgbm-example","text":"import lightgbm as lgb from wideboost.wrappers import wlgb from pydataset import data import numpy as np ######## ## Get and format the data DAT = np.asarray(data('Yogurt')) X = DAT[:,0:9] Y = np.zeros([X.shape[0],1]) Y[DAT[:,9] == 'dannon'] = 1 Y[DAT[:,9] == 'hiland'] = 2 Y[DAT[:,9] == 'weight'] = 3 n = X.shape[0] np.random.seed(123) train_idx = np.random.choice(np.arange(n),round(n*0.5),replace=False) test_idx = np.setdiff1d(np.arange(n),train_idx) train_data = lgb.Dataset(X[train_idx,:],label=Y[train_idx,0]) test_data = lgb.Dataset(X[test_idx,:],label=Y[test_idx,0]) ######### ######### ## Set parameters and run wide boosting param = {'btype':'I', ## wideboost param -- one of 'I', 'In', 'R', 'Rn' 'extra_dims':10, ## wideboost param -- integer >= 0 'objective':'multiclass', 'metric':'multi_error', 'num_class':4, 'learning_rate': 0.1 } wlgb_results = dict() bst = wlgb.train(param, train_data, valid_sets=test_data, num_boost_round=100, evals_result=wlgb_results)","title":"LightGBM Example"},{"location":"examples/#shap-example","text":"from wideboost.explainers.shap import WTreeExplainer import shap explainer = WTreeExplainer(bst) shap_values = explainer.shap_values(data('Yogurt').iloc[0:1,0:9]) shap.initjs() print(bst.predict(xgb.DMatrix(np.asarray(data('Yogurt'))[0:1,0:9]))) shap.force_plot(explainer.expected_value[3],shap_values[3][0,:],data('Yogurt').iloc[0,0:9]) Screenshot of output:","title":"SHAP Example"},{"location":"installation/","text":"Installation The wideboost python package can be installed from pypi via the following command. pip install wideboost wideboost wraps other Gradient Boosting packages. Because of this, it also requires at least one of the following packages to be installed. The installation instructions for those packages can be found on their pages. XGBoost LightGBM","title":"Installation"},{"location":"installation/#installation","text":"The wideboost python package can be installed from pypi via the following command. pip install wideboost wideboost wraps other Gradient Boosting packages. Because of this, it also requires at least one of the following packages to be installed. The installation instructions for those packages can be found on their pages. XGBoost LightGBM","title":"Installation"},{"location":"objectives/","text":"Objective Functions The objective functions listed here are available for both the XGBoost and LightGBM backends. We follow the naming conventions from both packages. When we provide an objective not available in a package currently (for example, multivariate squared error), we try to extend the naming conventions from either backend to cover this objective. Univariate Squared Error XGBoost - 'objective':'reg:squarederror' LightGBM - 'objective':'regression' Source code in wideboost/objectives/squareloss.py def squareloss_gradient_hessian ( X , B , Y ): # Loss = 1/2 (Y - X)^2 Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 R = Y - X . dot ( B ) dX = - R * B . transpose () dX2 = np . ones ( Y . shape ) * np . square ( B ) . transpose () return dX , dX2 Multivariate Squared Error XGBoost - 'objective':'multi:squarederror' LightGBM - 'objective':'multiregression' Source code in wideboost/objectives/squareloss.py def multi_squareloss_gradient_hessian ( X , B , Y ): # Loss = 1/2 (Y - X)^2 Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 # Take the gradient and hessian of the usual # boosting problem and pass through the general # converters in general_gh G = X . dot ( B ) - Y H = row_diag ( np . ones ([ Y . shape [ 0 ], Y . shape [ 1 ]])) dX = f_gradient_B ( G , B ) dX2 = f_hessian_B ( H , B ) return dX , dX2 Binary Classification XGBoost - 'objective':'binary:logistic' LightGBM - 'objective':'binary' Source code in wideboost/objectives/binarylogloss.py def binarylogloss_gradient_hessian ( X , B , Y ): # Loss = log(p) when Y == 1 # Loss = log(1-p) when Y == 0 # p = exp(XB)/(1 + exp(XB)) # 1-p = 1/(1 + exp(XB)) Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 logits = X . dot ( B ) P = 1 / ( 1 + np . exp ( - logits )) eps = 1e-16 dX = ( P - Y ) * B . transpose () dX2 = np . maximum (( P * ( 1 - P ) * np . square ( B ) . transpose () ), eps ) return dX , dX2 Multicategory Classification XGBoost - 'objective':'multi:softmax' LightGBM - 'objective':'multiclass' Source code in wideboost/objectives/categoricallogloss.py def categoricallogloss_gradient_hessian ( X , B , Y ): Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) def _onehot ( Y ): b = np . zeros (( Y . shape [ 0 ], Y . max () . astype ( int ) + 1 )) b [ np . arange ( Y . shape [ 0 ]), Y . astype ( int ) . flatten ()] = 1 return b assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 logits = X . dot ( B ) max_logit = np . max ( logits , axis = 1 , keepdims = True ) logits = logits - max_logit sum_exp = np . sum ( np . exp ( logits ), axis = 1 , keepdims = True ) P = np . exp ( logits ) / sum_exp eps = 1e-16 dX = ( P - _onehot ( Y )) . dot ( B . transpose ()) dX2 = np . maximum ( P . dot ( np . square ( B ) . transpose ()) - np . square ( P . dot ( B . transpose ())), eps ) return dX , dX2","title":"Objective Functions"},{"location":"objectives/#objective-functions","text":"The objective functions listed here are available for both the XGBoost and LightGBM backends. We follow the naming conventions from both packages. When we provide an objective not available in a package currently (for example, multivariate squared error), we try to extend the naming conventions from either backend to cover this objective.","title":"Objective Functions"},{"location":"objectives/#univariate-squared-error","text":"XGBoost - 'objective':'reg:squarederror' LightGBM - 'objective':'regression'","title":"Univariate Squared Error"},{"location":"objectives/#wideboost.objectives.squareloss.squareloss_gradient_hessian","text":"Source code in wideboost/objectives/squareloss.py def squareloss_gradient_hessian ( X , B , Y ): # Loss = 1/2 (Y - X)^2 Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 R = Y - X . dot ( B ) dX = - R * B . transpose () dX2 = np . ones ( Y . shape ) * np . square ( B ) . transpose () return dX , dX2","title":"wideboost.objectives.squareloss.squareloss_gradient_hessian"},{"location":"objectives/#multivariate-squared-error","text":"XGBoost - 'objective':'multi:squarederror' LightGBM - 'objective':'multiregression'","title":"Multivariate Squared Error"},{"location":"objectives/#wideboost.objectives.squareloss.multi_squareloss_gradient_hessian","text":"Source code in wideboost/objectives/squareloss.py def multi_squareloss_gradient_hessian ( X , B , Y ): # Loss = 1/2 (Y - X)^2 Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 # Take the gradient and hessian of the usual # boosting problem and pass through the general # converters in general_gh G = X . dot ( B ) - Y H = row_diag ( np . ones ([ Y . shape [ 0 ], Y . shape [ 1 ]])) dX = f_gradient_B ( G , B ) dX2 = f_hessian_B ( H , B ) return dX , dX2","title":"wideboost.objectives.squareloss.multi_squareloss_gradient_hessian"},{"location":"objectives/#binary-classification","text":"XGBoost - 'objective':'binary:logistic' LightGBM - 'objective':'binary'","title":"Binary Classification"},{"location":"objectives/#wideboost.objectives.binarylogloss.binarylogloss_gradient_hessian","text":"Source code in wideboost/objectives/binarylogloss.py def binarylogloss_gradient_hessian ( X , B , Y ): # Loss = log(p) when Y == 1 # Loss = log(1-p) when Y == 0 # p = exp(XB)/(1 + exp(XB)) # 1-p = 1/(1 + exp(XB)) Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 logits = X . dot ( B ) P = 1 / ( 1 + np . exp ( - logits )) eps = 1e-16 dX = ( P - Y ) * B . transpose () dX2 = np . maximum (( P * ( 1 - P ) * np . square ( B ) . transpose () ), eps ) return dX , dX2","title":"wideboost.objectives.binarylogloss.binarylogloss_gradient_hessian"},{"location":"objectives/#multicategory-classification","text":"XGBoost - 'objective':'multi:softmax' LightGBM - 'objective':'multiclass'","title":"Multicategory Classification"},{"location":"objectives/#wideboost.objectives.categoricallogloss.categoricallogloss_gradient_hessian","text":"Source code in wideboost/objectives/categoricallogloss.py def categoricallogloss_gradient_hessian ( X , B , Y ): Y = Y . reshape ([ Y . shape [ 0 ], - 1 ]) def _onehot ( Y ): b = np . zeros (( Y . shape [ 0 ], Y . max () . astype ( int ) + 1 )) b [ np . arange ( Y . shape [ 0 ]), Y . astype ( int ) . flatten ()] = 1 return b assert len ( X . shape ) == 2 assert len ( B . shape ) == 2 assert len ( Y . shape ) == 2 logits = X . dot ( B ) max_logit = np . max ( logits , axis = 1 , keepdims = True ) logits = logits - max_logit sum_exp = np . sum ( np . exp ( logits ), axis = 1 , keepdims = True ) P = np . exp ( logits ) / sum_exp eps = 1e-16 dX = ( P - _onehot ( Y )) . dot ( B . transpose ()) dX2 = np . maximum ( P . dot ( np . square ( B ) . transpose ()) - np . square ( P . dot ( B . transpose ())), eps ) return dX , dX2","title":"wideboost.objectives.categoricallogloss.categoricallogloss_gradient_hessian"},{"location":"overview/","text":"Overview Introduction Consider the usual Gradient Boosting (GB) problem. Given an input X X , GB attempts to relate X X to a desired output, Y Y , through a loss function, L L . Specifically, GB finds a function, F F , attempting to minimize: $$ L(Y,F(X)). $$ Wide Boosting (WB) solves a very similar problem. If F(X) \\in \\mathbb{R}^{1 \\times q} F(X) \\in \\mathbb{R}^{1 \\times q} and we consider a matrix, \\beta \\in \\mathbb{R}^{q \\times d} \\beta \\in \\mathbb{R}^{q \\times d} , then WB finds F F by attempting to minimize: $$ L(Y,F(X)\\beta). $$ The \\beta \\beta matrix is multiplied to the output, F(X) F(X) , exactly like you would find in a regression setup. The type of \\beta \\beta matrix we use can be set as a parameter in wideboost as described in Wideboost-specific Parameters Why multiply F F by a matrix? The \\beta \\beta multiplication allows F(X) F(X) to have a large (or small) dimension before it is compared to the output, Y Y . Analogously, a neural network with a \"wide\" hidden layer can have a much larger dimension than it's final output layer. In fact, Wide Boosting can be thought of as putting the usual GB function, F F , as the first layer in a wide, one-hidden-layer neural network. Just as a wide, one-hidden-layer neural network can outperform a narrow, one-hidden-layer neural network, a wide F F , fit via Wide Boosting, can outperform the more narrow F F that gets fit in a standard GB setup. Empirical performance on a handful of publicly available datasets is shown below. As you can see WB outperforms GB, as implemented in either XGBoost or LightGBM, on every dataset in this table. Implementation Given the simplicity of Wide Boosting, we are able to use world-leading GB packages such as XGBoost and LightGBM to fit WB models by simply providing those packages with the correct gradient and hessian calculations. If G G and H H are the gradients and hessians for F F in L(Y,F(X)) L(Y,F(X)) , then, the gradients and hessians for F F in L(Y,F(X)\\beta) L(Y,F(X)\\beta) are simply G \\beta^T G \\beta^T and \\beta H \\beta^T \\beta H \\beta^T . wideboost uses these formulas to provide both backend GB packages with gradient and hessian information so that we can find F F using the powerful boosting implementations found in both XGBoost and LightGBM .","title":"Overview"},{"location":"overview/#overview","text":"","title":"Overview"},{"location":"overview/#introduction","text":"Consider the usual Gradient Boosting (GB) problem. Given an input X X , GB attempts to relate X X to a desired output, Y Y , through a loss function, L L . Specifically, GB finds a function, F F , attempting to minimize: $$ L(Y,F(X)). $$ Wide Boosting (WB) solves a very similar problem. If F(X) \\in \\mathbb{R}^{1 \\times q} F(X) \\in \\mathbb{R}^{1 \\times q} and we consider a matrix, \\beta \\in \\mathbb{R}^{q \\times d} \\beta \\in \\mathbb{R}^{q \\times d} , then WB finds F F by attempting to minimize: $$ L(Y,F(X)\\beta). $$ The \\beta \\beta matrix is multiplied to the output, F(X) F(X) , exactly like you would find in a regression setup. The type of \\beta \\beta matrix we use can be set as a parameter in wideboost as described in Wideboost-specific Parameters","title":"Introduction"},{"location":"overview/#why-multiply-ff-by-a-matrix","text":"The \\beta \\beta multiplication allows F(X) F(X) to have a large (or small) dimension before it is compared to the output, Y Y . Analogously, a neural network with a \"wide\" hidden layer can have a much larger dimension than it's final output layer. In fact, Wide Boosting can be thought of as putting the usual GB function, F F , as the first layer in a wide, one-hidden-layer neural network. Just as a wide, one-hidden-layer neural network can outperform a narrow, one-hidden-layer neural network, a wide F F , fit via Wide Boosting, can outperform the more narrow F F that gets fit in a standard GB setup. Empirical performance on a handful of publicly available datasets is shown below. As you can see WB outperforms GB, as implemented in either XGBoost or LightGBM, on every dataset in this table.","title":"Why multiply FF by a matrix?"},{"location":"overview/#implementation","text":"Given the simplicity of Wide Boosting, we are able to use world-leading GB packages such as XGBoost and LightGBM to fit WB models by simply providing those packages with the correct gradient and hessian calculations. If G G and H H are the gradients and hessians for F F in L(Y,F(X)) L(Y,F(X)) , then, the gradients and hessians for F F in L(Y,F(X)\\beta) L(Y,F(X)\\beta) are simply G \\beta^T G \\beta^T and \\beta H \\beta^T \\beta H \\beta^T . wideboost uses these formulas to provide both backend GB packages with gradient and hessian information so that we can find F F using the powerful boosting implementations found in both XGBoost and LightGBM .","title":"Implementation"},{"location":"shap_explainer/","text":"SHAP Explainer SHAP Example from wideboost.explainers.shap import WTreeExplainer import shap explainer = WTreeExplainer(bst) shap_values = explainer.shap_values(data('Yogurt').iloc[0:1,0:9]) shap.initjs() print(bst.predict(xgb.DMatrix(np.asarray(data('Yogurt'))[0:1,0:9]))) shap.force_plot(explainer.expected_value[3],shap_values[3][0,:],data('Yogurt').iloc[0,0:9]) Screenshot of output:","title":"SHAP Explainer"},{"location":"shap_explainer/#shap-explainer","text":"","title":"SHAP Explainer"},{"location":"shap_explainer/#shap-example","text":"from wideboost.explainers.shap import WTreeExplainer import shap explainer = WTreeExplainer(bst) shap_values = explainer.shap_values(data('Yogurt').iloc[0:1,0:9]) shap.initjs() print(bst.predict(xgb.DMatrix(np.asarray(data('Yogurt'))[0:1,0:9]))) shap.force_plot(explainer.expected_value[3],shap_values[3][0,:],data('Yogurt').iloc[0,0:9]) Screenshot of output:","title":"SHAP Example"},{"location":"wideboost_parameters/","text":"Wideboost Parameters wideboost introduces two new parameters to Gradient Boosting: 'btype' and 'extra_dims' . The Overview describes Wide Boosting in more detail. The short summary is that Wide Boosting finds an F F to minimize the following expression: $$ L(Y,F(X)\\beta) $$ where L L is a loss function, X X is some inputs, Y Y is a known output and \\beta \\beta is a fixed matrix that gets initialized at the start of fitting F F . 'btype' The 'btype' parameter describes how \\beta \\beta is initialized. It has 4 options: 'R' fills \\beta \\beta with independent draws from a uniform random variable, U(0,1) U(0,1) . 'Rn' fills \\beta \\beta with independent draws from a uniform random variable, U(0,1) U(0,1) and normalizes each column of \\beta \\beta so that the columns of \\beta \\beta sum to one. 'I' fills the first several rows and columns of \\beta \\beta with an identity matrix. The remaining entries are independent draws from a uniform random variable, U(0,1) U(0,1) . 'I' fills the first several rows and columns of \\beta \\beta with an identity matrix. The remaining entries are independent draws from a uniform random variable, U(0,1) U(0,1) . The columns are then normalized to that each column of \\beta \\beta sums to one. Based on our empirical experiments, we haven't found a consistent pattern as to which 'btype' works best, so we recommend at least trying a couple to see which works. Performance of wideboost is not extremely sensitive to this parameter, but it can make a difference when trying to get the best possible performance. 'extra_dims' The 'extra_dims' parameter is an integer \\geq 0 \\geq 0 . It controls how \"wide\" F F is. If F(X) \\in \\mathbb{R}^{1\\times q} F(X) \\in \\mathbb{R}^{1\\times q} , \\beta \\in \\mathbb{R}^{q \\times d} \\beta \\in \\mathbb{R}^{q \\times d} and Y \\in \\mathbb{R}^d Y \\in \\mathbb{R}^d , the 'extra_dims' parameter determines how much larger q q is than d d . Specifically d + \\mbox{extra_dims} = q d + \\mbox{extra_dims} = q ; thus 'extra_dims' is exactly how many extra dimensions F(X) F(X) has compared to Y Y . Larger 'extra_dims' gives a wider model. We have found that wider models usually lead to better performance. However, given that the current backend packages, LightGBM and XGBoost , fit additional trees for every added dimension, increasing 'extra_dims' increases model fitting times. Notes If 'btype' = 'I' and 'extra_dims' = 0 , then Wide Boosting is equivalent to Gradient Boosting.","title":"Wideboost-specific Parameters"},{"location":"wideboost_parameters/#wideboost-parameters","text":"wideboost introduces two new parameters to Gradient Boosting: 'btype' and 'extra_dims' . The Overview describes Wide Boosting in more detail. The short summary is that Wide Boosting finds an F F to minimize the following expression: $$ L(Y,F(X)\\beta) $$ where L L is a loss function, X X is some inputs, Y Y is a known output and \\beta \\beta is a fixed matrix that gets initialized at the start of fitting F F .","title":"Wideboost Parameters"},{"location":"wideboost_parameters/#btype","text":"The 'btype' parameter describes how \\beta \\beta is initialized. It has 4 options: 'R' fills \\beta \\beta with independent draws from a uniform random variable, U(0,1) U(0,1) . 'Rn' fills \\beta \\beta with independent draws from a uniform random variable, U(0,1) U(0,1) and normalizes each column of \\beta \\beta so that the columns of \\beta \\beta sum to one. 'I' fills the first several rows and columns of \\beta \\beta with an identity matrix. The remaining entries are independent draws from a uniform random variable, U(0,1) U(0,1) . 'I' fills the first several rows and columns of \\beta \\beta with an identity matrix. The remaining entries are independent draws from a uniform random variable, U(0,1) U(0,1) . The columns are then normalized to that each column of \\beta \\beta sums to one. Based on our empirical experiments, we haven't found a consistent pattern as to which 'btype' works best, so we recommend at least trying a couple to see which works. Performance of wideboost is not extremely sensitive to this parameter, but it can make a difference when trying to get the best possible performance.","title":"'btype'"},{"location":"wideboost_parameters/#extra_dims","text":"The 'extra_dims' parameter is an integer \\geq 0 \\geq 0 . It controls how \"wide\" F F is. If F(X) \\in \\mathbb{R}^{1\\times q} F(X) \\in \\mathbb{R}^{1\\times q} , \\beta \\in \\mathbb{R}^{q \\times d} \\beta \\in \\mathbb{R}^{q \\times d} and Y \\in \\mathbb{R}^d Y \\in \\mathbb{R}^d , the 'extra_dims' parameter determines how much larger q q is than d d . Specifically d + \\mbox{extra_dims} = q d + \\mbox{extra_dims} = q ; thus 'extra_dims' is exactly how many extra dimensions F(X) F(X) has compared to Y Y . Larger 'extra_dims' gives a wider model. We have found that wider models usually lead to better performance. However, given that the current backend packages, LightGBM and XGBoost , fit additional trees for every added dimension, increasing 'extra_dims' increases model fitting times.","title":"'extra_dims'"},{"location":"wideboost_parameters/#notes","text":"If 'btype' = 'I' and 'extra_dims' = 0 , then Wide Boosting is equivalent to Gradient Boosting.","title":"Notes"},{"location":"wrapper_lightgbm/","text":"LightGBM Wrapper train -- Trains a wideboost model using the lightGBM backend. Parameters: Name Type Description Default param list Named parameter list. Uses lightGBM conventions. Requires two parameters in addition to the usual lightGBM parameters, 'btype' and 'extra_dims'. required Returns: Type Description wlgb A wlgb object containing the lightGBM object with objective and evaluation objects. Source code in wideboost/wrappers/wlgb.py def train ( param , train_set , num_boost_round = 100 , valid_sets = None , valid_names = None , fobj = None , feval = None , init_model = None , feature_name = 'auto' , categorical_feature = 'auto' , early_stopping_rounds = None , evals_result = None , verbose_eval = True , learning_rates = None , keep_training_booster = False , callbacks = None ): \"\"\"train -- Trains a wideboost model using the lightGBM backend. Args: param (list): Named parameter list. Uses lightGBM conventions. Requires two parameters in addition to the usual lightGBM parameters, 'btype' and 'extra_dims'. Returns: wlgb: A wlgb object containing the lightGBM object with objective and evaluation objects. \"\"\" params = param . copy () if not isinstance ( fobj , lgb_objective ): assert params [ 'extra_dims' ] >= 0 else : print ( \"Using custom objective. Removed extra_dims restriction.\" ) # Overwrite needed params print ( \"Overwriting param `num_class`\" ) try : # TODO this is ugly nclass = params [ \"num_class\" ] except : params [ 'num_class' ] = 1 if isinstance ( fobj , lgb_objective ): print ( \"Found custom wideboost-compatible objective. Using user specified objective.\" ) else : fobj = get_objective ( params ) params [ 'num_class' ] = params [ 'num_class' ] + params [ 'extra_dims' ] params . pop ( 'extra_dims' ) print ( \"Overwriting param `objective` while setting `fobj` in train.\" ) params [ 'objective' ] = 'regression' try : feval = get_eval_metric ( params , fobj ) params . pop ( 'metric' ) print ( \"Moving param `metric` to an feval.\" ) except : None lgbobject = lgb . train ( params , train_set , num_boost_round = num_boost_round , valid_sets = valid_sets , valid_names = valid_names , fobj = fobj , feval = feval , init_model = init_model , feature_name = 'auto' , categorical_feature = 'auto' , early_stopping_rounds = early_stopping_rounds , evals_result = evals_result , verbose_eval = verbose_eval , learning_rates = learning_rates , keep_training_booster = keep_training_booster , callbacks = callbacks ) return wlgb ( lgbobject , fobj , feval )","title":"LightGBM wrapper"},{"location":"wrapper_lightgbm/#lightgbm-wrapper","text":"","title":"LightGBM Wrapper"},{"location":"wrapper_lightgbm/#wideboost.wrappers.wlgb.train","text":"train -- Trains a wideboost model using the lightGBM backend. Parameters: Name Type Description Default param list Named parameter list. Uses lightGBM conventions. Requires two parameters in addition to the usual lightGBM parameters, 'btype' and 'extra_dims'. required Returns: Type Description wlgb A wlgb object containing the lightGBM object with objective and evaluation objects. Source code in wideboost/wrappers/wlgb.py def train ( param , train_set , num_boost_round = 100 , valid_sets = None , valid_names = None , fobj = None , feval = None , init_model = None , feature_name = 'auto' , categorical_feature = 'auto' , early_stopping_rounds = None , evals_result = None , verbose_eval = True , learning_rates = None , keep_training_booster = False , callbacks = None ): \"\"\"train -- Trains a wideboost model using the lightGBM backend. Args: param (list): Named parameter list. Uses lightGBM conventions. Requires two parameters in addition to the usual lightGBM parameters, 'btype' and 'extra_dims'. Returns: wlgb: A wlgb object containing the lightGBM object with objective and evaluation objects. \"\"\" params = param . copy () if not isinstance ( fobj , lgb_objective ): assert params [ 'extra_dims' ] >= 0 else : print ( \"Using custom objective. Removed extra_dims restriction.\" ) # Overwrite needed params print ( \"Overwriting param `num_class`\" ) try : # TODO this is ugly nclass = params [ \"num_class\" ] except : params [ 'num_class' ] = 1 if isinstance ( fobj , lgb_objective ): print ( \"Found custom wideboost-compatible objective. Using user specified objective.\" ) else : fobj = get_objective ( params ) params [ 'num_class' ] = params [ 'num_class' ] + params [ 'extra_dims' ] params . pop ( 'extra_dims' ) print ( \"Overwriting param `objective` while setting `fobj` in train.\" ) params [ 'objective' ] = 'regression' try : feval = get_eval_metric ( params , fobj ) params . pop ( 'metric' ) print ( \"Moving param `metric` to an feval.\" ) except : None lgbobject = lgb . train ( params , train_set , num_boost_round = num_boost_round , valid_sets = valid_sets , valid_names = valid_names , fobj = fobj , feval = feval , init_model = init_model , feature_name = 'auto' , categorical_feature = 'auto' , early_stopping_rounds = early_stopping_rounds , evals_result = evals_result , verbose_eval = verbose_eval , learning_rates = learning_rates , keep_training_booster = keep_training_booster , callbacks = callbacks ) return wlgb ( lgbobject , fobj , feval )","title":"wideboost.wrappers.wlgb.train"},{"location":"wrapper_xgboost/","text":"XGBoost Wrapper train -- Trains a wideboost model using the XGBoost backend. Parameters: Name Type Description Default param list Named parameter list. Uses XGBoost conventions. Requires two parameters in addition to the usual XGBoost parameters, 'btype' and 'extra_dims'. required Returns: Type Description wxgb A wxgb object containing the XGBoost object with objective and evaluation objects. Source code in wideboost/wrappers/wxgb.py def train ( param , dtrain , num_boost_round = 10 , evals = (), obj = None , feval = None , maximize = False , early_stopping_rounds = None , evals_result = None , verbose_eval = True , xgb_model = None , callbacks = None ): \"\"\"train -- Trains a wideboost model using the XGBoost backend. Args: param (list): Named parameter list. Uses XGBoost conventions. Requires two parameters in addition to the usual XGBoost parameters, 'btype' and 'extra_dims'. Returns: wxgb: A wxgb object containing the XGBoost object with objective and evaluation objects. \"\"\" params = param . copy () if not isinstance ( obj , xgb_objective ): assert params [ 'extra_dims' ] >= 0 else : print ( \"Using custom objective. Removed extra_dims restriction.\" ) # Overwrite needed params print ( \"Overwriting param `num_class`\" ) try : # TODO this is ugly nclass = params [ \"num_class\" ] except : params [ 'num_class' ] = 1 if isinstance ( obj , xgb_objective ): print ( \"Found custom wideboost-compatible objective. Using user specified objective.\" ) else : obj = get_objective ( params ) params [ 'num_class' ] = params [ 'num_class' ] + params [ 'extra_dims' ] params . pop ( 'extra_dims' ) print ( \"Overwriting param `objective` while setting `obj` in train.\" ) params [ 'objective' ] = 'reg:squarederror' try : feval = get_eval_metric ( params , obj ) params . pop ( 'eval_metric' ) print ( \"Moving param `eval_metric` to an feval.\" ) except : None print ( \"Setting param `disable_default_eval_metric` to 1.\" ) params [ 'disable_default_eval_metric' ] = 1 # TODO: base_score should be set depending on the objective chosen # TODO: Allow some items to be overwritten by user. This being one of them. params [ 'base_score' ] = 0.0 xgbobject = xgb . train ( params , dtrain , num_boost_round = num_boost_round , evals = evals , obj = obj , feval = feval , maximize = maximize , early_stopping_rounds = early_stopping_rounds , evals_result = evals_result , verbose_eval = verbose_eval , xgb_model = xgb_model , callbacks = callbacks ) return wxgb ( xgbobject , obj , feval )","title":"XGBoost wrapper"},{"location":"wrapper_xgboost/#xgboost-wrapper","text":"","title":"XGBoost Wrapper"},{"location":"wrapper_xgboost/#wideboost.wrappers.wxgb.train","text":"train -- Trains a wideboost model using the XGBoost backend. Parameters: Name Type Description Default param list Named parameter list. Uses XGBoost conventions. Requires two parameters in addition to the usual XGBoost parameters, 'btype' and 'extra_dims'. required Returns: Type Description wxgb A wxgb object containing the XGBoost object with objective and evaluation objects. Source code in wideboost/wrappers/wxgb.py def train ( param , dtrain , num_boost_round = 10 , evals = (), obj = None , feval = None , maximize = False , early_stopping_rounds = None , evals_result = None , verbose_eval = True , xgb_model = None , callbacks = None ): \"\"\"train -- Trains a wideboost model using the XGBoost backend. Args: param (list): Named parameter list. Uses XGBoost conventions. Requires two parameters in addition to the usual XGBoost parameters, 'btype' and 'extra_dims'. Returns: wxgb: A wxgb object containing the XGBoost object with objective and evaluation objects. \"\"\" params = param . copy () if not isinstance ( obj , xgb_objective ): assert params [ 'extra_dims' ] >= 0 else : print ( \"Using custom objective. Removed extra_dims restriction.\" ) # Overwrite needed params print ( \"Overwriting param `num_class`\" ) try : # TODO this is ugly nclass = params [ \"num_class\" ] except : params [ 'num_class' ] = 1 if isinstance ( obj , xgb_objective ): print ( \"Found custom wideboost-compatible objective. Using user specified objective.\" ) else : obj = get_objective ( params ) params [ 'num_class' ] = params [ 'num_class' ] + params [ 'extra_dims' ] params . pop ( 'extra_dims' ) print ( \"Overwriting param `objective` while setting `obj` in train.\" ) params [ 'objective' ] = 'reg:squarederror' try : feval = get_eval_metric ( params , obj ) params . pop ( 'eval_metric' ) print ( \"Moving param `eval_metric` to an feval.\" ) except : None print ( \"Setting param `disable_default_eval_metric` to 1.\" ) params [ 'disable_default_eval_metric' ] = 1 # TODO: base_score should be set depending on the objective chosen # TODO: Allow some items to be overwritten by user. This being one of them. params [ 'base_score' ] = 0.0 xgbobject = xgb . train ( params , dtrain , num_boost_round = num_boost_round , evals = evals , obj = obj , feval = feval , maximize = maximize , early_stopping_rounds = early_stopping_rounds , evals_result = evals_result , verbose_eval = verbose_eval , xgb_model = xgb_model , callbacks = callbacks ) return wxgb ( xgbobject , obj , feval )","title":"wideboost.wrappers.wxgb.train"}]}